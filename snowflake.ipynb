{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dea7c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                  \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                  \"Chrome/123.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def getSp500tickers() :\n",
    "    import pandas as pd\n",
    "    import requests\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status() \n",
    "\n",
    "    # now feed HTML text directly to pandas\n",
    "    tables = pd.read_html(response.text)\n",
    "    sp500_df = tables[0]\n",
    "    symbols = sp500_df['Symbol'].str.replace('.', '-', regex=False)\n",
    "    return symbols.tolist()[:5] \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_dataFromYf(ti):\n",
    "    import yfinance as yf\n",
    "    symbols = ti.xcom_pull(task_ids='extract_task_first') \n",
    "    results = {}\n",
    "    for symbol in symbols:  \n",
    "        try :\n",
    "            data = yf.download(\n",
    "                tickers=symbol,        # one ticker or list of tickers\n",
    "                start='2025-10-10',    # start date (string or datetime)\n",
    "                end='2025-10-11',      # end date\n",
    "                interval='1d',         # data frequency: 1m, 2m, 5m, 1h, 1d, 1wk, 1mo, etc.\n",
    "                auto_adjust=True,      # adjust for splits/dividends\n",
    "                progress=False,        # show download progress\n",
    "                ignore_tz= True                  \n",
    "                )\n",
    "            if not data.empty:\n",
    "                results[symbol] = data\n",
    "            else :\n",
    "                results[symbol] = {}\n",
    "        except Exception as e:\n",
    "            results[symbol] = {\"error\": str(e)}\n",
    "        # finally :\n",
    "        #     results[symbol] = data.reset_index().to_dict(orient='list') # did this as gpt told that data itself is just a python dict which will give serialization error in Airflow\n",
    "    return results        \n",
    "# symbols = getSp500tickers()\n",
    "# print(get_dataFromYf(symbols[:5],'2025-10-10','2025-10-11','1d'))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa0f741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f5e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(ti):\n",
    "    import pandas as pd, os\n",
    "    from datetime import datetime\n",
    "\n",
    "    map_data = ti.xcom_pull(task_ids='extract_task_second')\n",
    "    results = []\n",
    "\n",
    "    for key, val in map_data.items():\n",
    "        val = pd.DataFrame(val)\n",
    "        val[\"Symbol\"] = key\n",
    "        val[\"close_change\"] = val['Close'].diff().fillna(0)\n",
    "        val[\"close_pct_change\"] = val['Close'].pct_change().fillna(0) * 100\n",
    "        selected = val[['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                        'close_change', 'close_pct_change']]\n",
    "        results.append(selected)\n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    out_csv = os.path.join(\"/tmp\", f\"sp500_transformed_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.csv\")\n",
    "    final_df.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved transformed data to {out_csv}\")\n",
    "    return {\n",
    "        \"csv_path\": out_csv,\n",
    "        \"data\": final_df.to_dict(orient=\"records\")  # serializable object\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15142ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "aws_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "snowflake_user = os.getenv(\"SNOWFLAKE_USER\")\n",
    "\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "\n",
    "def load_to_s3(ti):\n",
    "    file = ti.xcom_pull(task_ids='transform_task')\n",
    "    csv_path = file.get('csv_path') if file else None\n",
    "    if not csv_path or not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV not found at {csv_path}\")\n",
    "\n",
    "    from datetime import datetime\n",
    "    import os\n",
    "\n",
    "    # reuse the timestamped filename written by transform()\n",
    "    basename = os.path.basename(csv_path)  # e.g. sp500_transformed_20251019T120000Z.csv\n",
    "    s3_key = f\"airflow_dag_project/sp500_data/{basename}\"\n",
    "    s3_hook = S3Hook(aws_conn_id='aws_default')\n",
    "    s3_hook.load_file(\n",
    "        filename=csv_path,\n",
    "        key=s3_key,\n",
    "        bucket_name='hassan-snowflake-class-project',\n",
    "        replace=False  \n",
    "    )\n",
    "    return {\n",
    "        \"s3_path\": f\"s3://hassan-snowflake-class-project/{s3_key}\",\n",
    "        \"csv_path\": csv_path\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def load_to_snowflake(ti):\n",
    "    # pull csv_path from transform_task XCom (same file written in container)\n",
    "    file = ti.xcom_pull(task_ids='transform_task')\n",
    "    csv_path = file.get('csv_path') if file else None\n",
    "    if not csv_path or not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV not found at {csv_path}\")\n",
    "\n",
    "    snowflake_hook = SnowflakeHook(snowflake_conn_id='snowflake_default')\n",
    "    conn = snowflake_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # target identifiers from env (set in your Docker container or Airflow env)\n",
    "    db = \"AIRFLOW_ETL_DATABASE\"\n",
    "    schema = \"PUBLIC\"\n",
    "    table = \"SP500_TABLE\"\n",
    "    if not db or not schema:\n",
    "        raise RuntimeError(\"AIRFLOW_ETL_DATABASE or SNOWFLAKE_SCHEMA not set in env\")\n",
    "\n",
    "    target_table_fqn = f\"{db}.{schema}.{table}\"\n",
    "\n",
    "    try:\n",
    "        # create or use a temporary stage (this will be session-scoped)\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE OR REPLACE TEMPORARY STAGE airflow_stage\n",
    "            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "        \"\"\")\n",
    "\n",
    "        # PUT the local file into the temporary stage (requires connector support for PUT)\n",
    "        # Using @airflow_stage uploads file to that temporary stage\n",
    "        put_cmd = f\"PUT file://{csv_path} @airflow_stage AUTO_COMPRESS=FALSE;\"\n",
    "        cursor.execute(put_cmd)\n",
    "\n",
    "        # COPY INTO the target table from the temporary stage\n",
    "        copy_cmd = f\"\"\"\n",
    "            COPY INTO {target_table_fqn}\n",
    "            FROM @airflow_stage\n",
    "            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1)\n",
    "            ON_ERROR = 'CONTINUE';\n",
    "        \"\"\"\n",
    "        cursor.execute(copy_cmd)\n",
    "\n",
    "        conn.commit()\n",
    "        print(\"âœ… Data uploaded to Snowflake (PUT -> COPY successful).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Snowflake load failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79895b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.standard.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "with DAG(\n",
    "    dag_id='SP500_Symbol_data_DAG',\n",
    "    description='ETL for S&P 500 data to transform and prepare for load',\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule_interval='@daily',\n",
    "    catchup=False,\n",
    "    default_args={'retries': 1, 'retry_delay': timedelta(minutes=5)},\n",
    "    tags=['sp500', 'etl', 'finance'],\n",
    ") as dag:\n",
    "\n",
    "    extract_task_first = PythonOperator(\n",
    "        task_id='extract_task_first',\n",
    "        python_callable=getSp500tickers\n",
    "    )\n",
    "\n",
    "    extract_task_second = PythonOperator(\n",
    "        task_id='extract_task_second',\n",
    "        python_callable=get_dataFromYf\n",
    "    )\n",
    "\n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_task',\n",
    "        python_callable=transform\n",
    "    )\n",
    "    load_task_s3 = PythonOperator(\n",
    "        task_id='load_task_s3',\n",
    "        python_callable=load_to_s3\n",
    "    )\n",
    "    load_task_Snowflake = PythonOperator(\n",
    "        task_id='load_task_Snowflake',\n",
    "        python_callable=load_to_snowflake\n",
    "    )\n",
    "\n",
    "    extract_task_first >> extract_task_second >> transform_task >> [load_task_s3, load_task_Snowflake]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b8d43",
   "metadata": {},
   "source": [
    "# ðŸ§  ETL Debug History â€” S&P 500 DAG to Local Script\n",
    "\n",
    "## ðŸ“… Context\n",
    "This document captures all major issues, bugs, and debugging steps we encountered while building and stabilizing the **S&P 500 Data ETL Pipeline** (Airflow â†’ S3 â†’ Snowflake). Each error is documented with cause, diagnosis, and resolution.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© 1. XCom Payload Overflow\n",
    "\n",
    "**Error:**\n",
    "```python\n",
    "airflow.exceptions.XComSizeExceeded: XCom payload too large (>48KB)\n",
    "\n",
    "Cause: We were passing the entire Yahoo Finance DataFrame object between tasks via XCom. Airflow's metadata DB has a 48 KB limit for XCom payloads.\n",
    "\n",
    "Fix: Instead of pushing full dataframes, we saved the CSV to /tmp and pushed only the file path through XCom\n",
    "\n",
    "output_csv = \"/tmp/sp500_yf_data/sp500_combined.csv\"\n",
    "ti.xcom_push(key=\"data_path\", value=output_csv) \n",
    "\n",
    "Date,Close,High,Low,Open,Volume,Symbol,Close,High,Low,Open,Volume,...\n",
    ",MMM,MMM,MMM,MMM,MMM,,AOS,AOS,...\n",
    "\n",
    "Cause: You concatenated multiple tickers horizontally instead of vertically.\n",
    "\n",
    "Fix:all_data.append(data)\n",
    "combined_df = pd.concat(all_data, ignore_index=True) \n",
    "\n",
    "Now each symbol adds rows, not extra columns.\n",
    "\n",
    "ðŸ“‰ 4. Snowflake Load Error: Date 'MMM' is not recognized\n",
    "Error:\n",
    "\n",
    "text\n",
    "Status: Failed\n",
    "Error: Date 'MMM' is not recognized\n",
    "Column: \"SP500_TABLE\"[\"DATE\":2]\n",
    "Diagnosis: The first CSV line had column headers followed by \"MMM\" under the Date column â€” meaning the column order in CSV didn't match the Snowflake table definition.\n",
    "\n",
    "Cause: Snowflake assumed your second column (Symbol) was actually the Date field.\n",
    "\n",
    "Fix: We fixed both CSV generation and table definition alignment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99793917",
   "metadata": {},
   "source": [
    "Old schema:\n",
    "\n",
    "sql\n",
    "CREATE TABLE SP500_TABLE (\n",
    "    SYMBOL VARCHAR,\n",
    "    DATE DATE,\n",
    "    OPEN FLOAT,\n",
    "    HIGH FLOAT,\n",
    "    LOW FLOAT,\n",
    "    CLOSE FLOAT,\n",
    "    VOLUME FLOAT,\n",
    "    CLOSE_CHANGE FLOAT,\n",
    "    CLOSE_PCT_CHANGE FLOAT\n",
    ");\n",
    "Problem: The CSV had the order: Date, Symbol, Open, High, Low, Close, Volume, close_change, close_pct_change\n",
    "\n",
    "Fix:\n",
    "\n",
    "sql\n",
    "CREATE OR REPLACE TABLE AIRFLOW_ETL_DATABASE.PUBLIC.SP500_TABLE (\n",
    "    DATE DATE,\n",
    "    SYMBOL VARCHAR(16777216),\n",
    "    OPEN FLOAT,\n",
    "    HIGH FLOAT,\n",
    "    LOW FLOAT,\n",
    "    CLOSE FLOAT,\n",
    "    VOLUME FLOAT,\n",
    "    CLOSE_CHANGE FLOAT,\n",
    "    CLOSE_PCT_CHANGE FLOAT\n",
    ");\n",
    "âœ… Result: Snowflake started parsing rows correctly without misinterpreting 'MMM' as a date.\n",
    "\n",
    "ðŸ“¦ 6. Stage Confusion in Snowflake\n",
    "Issue: You didn't see the stage (airflow_stage) in Snowflake UI.\n",
    "\n",
    "Cause: We used a temporary stage:\n",
    "\n",
    "sql\n",
    "CREATE OR REPLACE TEMPORARY STAGE airflow_stage\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "Temporary stages are session-scoped and disappear once the connection closes â€” they won't appear in the UI.\n",
    "\n",
    "Fix: For a persistent stage:\n",
    "\n",
    "sql\n",
    "CREATE OR REPLACE STAGE airflow_stage\n",
    "FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "ðŸ§® 7. Field Delimiter Missing\n",
    "Error: Snowflake read the entire line as one string column.\n",
    "\n",
    "Cause: The COPY INTO command lacked explicit FIELD_DELIMITER.\n",
    "\n",
    "Fix:\n",
    "\n",
    "sql\n",
    "FILE_FORMAT = (\n",
    "  TYPE = 'CSV'\n",
    "  FIELD_DELIMITER = ','\n",
    "  FIELD_OPTIONALLY_ENCLOSED_BY='\"'\n",
    "  SKIP_HEADER=1\n",
    ");\n",
    "ðŸ•’ 8. Output File Overwrites\n",
    "Issue: Each DAG run overwrote the same transformed file.\n",
    "\n",
    "Fix:\n",
    "\n",
    "python\n",
    "output_file = f\"/tmp/sp500_yf_data/sp500_transformed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ef4288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Starting Local ETL Pipeline (No Airflow)...\n",
      "\n",
      "ðŸ” Fetching S&P 500 tickers from Wikipedia...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hhhh\\AppData\\Local\\Temp\\ipykernel_11304\\3261618870.py:28: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n",
      "  tables = pd.read_html(response.text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Retrieved 5 tickers: ['MMM', 'AOS', 'ABT', 'ABBV', 'ACN']\n",
      "â¬‡ï¸ Downloading data from Yahoo Finance...\n",
      "âœ… Clean combined CSV saved at: ./tmp/sp500_yf_data\\sp500_combined.csv\n",
      "âš™ï¸ Transforming data...\n",
      "âœ… Transformation complete â†’ ./tmp/sp500_yf_data/sp500_transformed.csv\n",
      "ðŸ“¤ (Mock) Uploading ./tmp/sp500_yf_data/sp500_transformed.csv to S3...\n",
      "âœ… Mock upload complete: s3://hassan-snowflake-class-project/airflow_dag_project/sp500_data/sp500_transformed.csv\n",
      "ðŸ“¦ (Mock) Loading ./tmp/sp500_yf_data/sp500_transformed.csv to Snowflake table...\n",
      "âœ… Mock load to Snowflake complete.\n",
      "\n",
      "âœ… Local ETL Pipeline Completed Successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------------------------\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 1 â€” Get S&P 500 Symbols\n",
    "# -------------------------------------------------------------------\n",
    "def getSp500tickers():\n",
    "    print(\"ðŸ”¹ Fetching S&P 500 tickers...\")\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tables = pd.read_html(response.text)\n",
    "    sp500_df = tables[0]\n",
    "    symbols = sp500_df[\"Symbol\"].str.replace(\".\", \"-\", regex=False)\n",
    "    print(f\"âœ… Got {len(symbols)} tickers.\")\n",
    "    return symbols.tolist()[:5]  # for local testing\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 2 â€” Extract Data via yfinance\n",
    "# -------------------------------------------------------------------\n",
    "def get_dataFromYf(symbols):\n",
    "    print(\"ðŸ”¹ Downloading Yahoo Finance data...\")\n",
    "    output_dir = \"sp500_yf_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_data = []\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            data = yf.download(\n",
    "                tickers=symbol,\n",
    "                start=\"2025-10-10\",\n",
    "                end=\"2025-10-11\",\n",
    "                interval=\"1d\",\n",
    "                auto_adjust=True,\n",
    "                progress=False,\n",
    "                ignore_tz=True,\n",
    "            )\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = [col[0] if isinstance(col, tuple) else col for col in data.columns]\n",
    "\n",
    "            data.reset_index(inplace=True)\n",
    "            data[\"Symbol\"] = symbol\n",
    "            data = data[[\"Date\", \"Symbol\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "            all_data.append(data)\n",
    "            print(f\"âœ… {symbol} done\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error fetching {symbol}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        raise RuntimeError(\"No data downloaded from yfinance.\")\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    output_csv = os.path.join(output_dir, \"sp500_combined.csv\")\n",
    "    combined_df.to_csv(output_csv, index=False)\n",
    "    print(f\"âœ… Combined CSV saved at: {output_csv}\")\n",
    "    return output_csv\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 3 â€” Transform\n",
    "# -------------------------------------------------------------------\n",
    "def transform(csv_path):\n",
    "    print(f\"ðŸ”¹ Transforming data: {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    df[\"close_change\"] = df.groupby(\"Symbol\")[\"Close\"].diff()\n",
    "    df[\"close_pct_change\"] = df.groupby(\"Symbol\")[\"Close\"].pct_change() * 100\n",
    "\n",
    "    output_file = f\"sp500_yf_data/sp500_transformed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"âœ… Transformation complete â†’ {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 4 â€” Load to S3\n",
    "# -------------------------------------------------------------------\n",
    "def load_to_s3(csv_path):\n",
    "    print(f\"ðŸ”¹ Uploading to S3: {csv_path}\")\n",
    "    s3_hook = S3Hook(aws_conn_id=\"aws_default\")\n",
    "    s3_key = f\"airflow_dag_project/sp500_data/{os.path.basename(csv_path)}\"\n",
    "    s3_hook.load_file(\n",
    "        filename=csv_path,\n",
    "        key=s3_key,\n",
    "        bucket_name=\"hassan-snowflake-class-project\",\n",
    "        replace=True,\n",
    "    )\n",
    "    print(f\"âœ… Uploaded to s3://hassan-snowflake-class-project/{s3_key}\")\n",
    "    return s3_key\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 5 â€” Load to Snowflake\n",
    "# -------------------------------------------------------------------\n",
    "def load_to_snowflake(csv_path):\n",
    "    print(f\"ðŸ”¹ Loading {csv_path} into Snowflake...\")\n",
    "\n",
    "    snowflake_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_default\")\n",
    "    conn = snowflake_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    db = \"AIRFLOW_ETL_DATABASE\"\n",
    "    schema = \"PUBLIC\"\n",
    "    table = \"SP500_TABLE\"\n",
    "    target_table_fqn = f\"{db}.{schema}.{table}\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE OR REPLACE TEMPORARY STAGE airflow_stage\n",
    "            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "        \"\"\")\n",
    "        cursor.execute(f\"PUT file://{csv_path} @airflow_stage AUTO_COMPRESS=FALSE;\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            COPY INTO {target_table_fqn}\n",
    "            FROM @airflow_stage\n",
    "            FILE_FORMAT = (TYPE = 'CSV' \n",
    "                FIELD_DELIMITER = ',' \n",
    "                FIELD_OPTIONALLY_ENCLOSED_BY='\"' \n",
    "                SKIP_HEADER=1)\n",
    "            ON_ERROR = 'CONTINUE';\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        print(\"âœ… Data uploaded to Snowflake successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Snowflake load failed: {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MAIN EXECUTION FLOW\n",
    "# -------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    symbols = getSp500tickers()\n",
    "    csv_path = get_dataFromYf(symbols)\n",
    "    transformed_csv = transform(csv_path)\n",
    "    load_to_s3(transformed_csv)\n",
    "    load_to_snowflake(transformed_csv)\n",
    "    print(\"\\nðŸš€ ETL Process Complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4879232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.providers.standard.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.hooks.s3 import S3Hook\n",
    "from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import yfinance as yf\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# CONFIG\n",
    "# -------------------------------------------------------------------\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/123.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 1 â€” Get S&P 500 Symbols\n",
    "# -------------------------------------------------------------------\n",
    "def getSp500tickers():\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    tables = pd.read_html(response.text)\n",
    "    sp500_df = tables[0]\n",
    "    symbols = sp500_df[\"Symbol\"].str.replace(\".\", \"-\", regex=False)\n",
    "    return symbols.tolist()  # limit for testing\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 2 â€” Extract Data via yfinance\n",
    "# -------------------------------------------------------------------\n",
    "def get_dataFromYf(ti):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import yfinance as yf\n",
    "\n",
    "    symbols = ti.xcom_pull(task_ids=\"extract_task_first\")\n",
    "    if not symbols:\n",
    "        raise ValueError(\"âŒ No symbols received from previous task\")\n",
    "\n",
    "    output_dir = \"/tmp/sp500_yf_data\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    for symbol in symbols:\n",
    "        try:\n",
    "            data = yf.download(\n",
    "                tickers=symbol,\n",
    "                start=\"2025-10-10\",\n",
    "                end=\"2025-10-11\",\n",
    "                interval=\"1d\",\n",
    "                auto_adjust=True,\n",
    "                progress=False,\n",
    "                ignore_tz=True,\n",
    "            )\n",
    "            if isinstance(data.columns, pd.MultiIndex):\n",
    "                data.columns = [col[0] if isinstance(col, tuple) else col for col in data.columns]\n",
    "\n",
    "            data.reset_index(inplace=True)\n",
    "            data[\"Symbol\"] = symbol\n",
    "\n",
    "            # âœ… Force correct order and only keep the essentials\n",
    "            data = data[[\"Date\", \"Symbol\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "            all_data.append(data)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error fetching {symbol}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        raise RuntimeError(\"No data downloaded from yfinance.\")\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    output_csv = os.path.join(output_dir, \"sp500_combined.csv\")\n",
    "    combined_df.to_csv(output_csv, index=False)\n",
    "    print(f\"âœ… Combined CSV saved at: {output_csv}\")\n",
    "    return output_csv\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 3 â€” Transform\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def transform(ti):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "\n",
    "    # Pull single CSV path from previous task\n",
    "    csv_path = ti.xcom_pull(task_ids=\"extract_task_second\")\n",
    "    if not csv_path or not os.path.exists(csv_path):\n",
    "        raise ValueError(f\"âŒ Invalid path received from extractor: {csv_path}\")\n",
    "\n",
    "    print(f\"âœ… Reading combined CSV: {csv_path}\")\n",
    "\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Convert columns to numeric safely\n",
    "    for col in [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    # Compute per-symbol close changes\n",
    "    df[\"close_change\"] = df.groupby(\"Symbol\")[\"Close\"].diff()\n",
    "    df[\"close_pct_change\"] = df.groupby(\"Symbol\")[\"Close\"].pct_change() * 100\n",
    "\n",
    "    # Save transformed CSV\n",
    "    output_file = f\"/tmp/sp500_yf_data/sp500_transformed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"âœ… Transformation complete â†’ {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 4 â€” Load to S3\n",
    "# -------------------------------------------------------------------\n",
    "def load_to_s3(ti):\n",
    "    csv_path = ti.xcom_pull(task_ids=\"transform_task\")\n",
    "    if not csv_path or not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV not found at {csv_path}\")\n",
    "\n",
    "    basename = os.path.basename(csv_path)\n",
    "    s3_key = f\"airflow_dag_project/sp500_data/{basename}\"\n",
    "\n",
    "    s3_hook = S3Hook(aws_conn_id=\"aws_default\")\n",
    "    s3_hook.load_file(\n",
    "        filename=csv_path,\n",
    "        key=s3_key,\n",
    "        bucket_name=\"hassan-snowflake-class-project\",\n",
    "        replace=True,\n",
    "    )\n",
    "\n",
    "    return {\"s3_path\": f\"s3://hassan-snowflake-class-project/{s3_key}\", \"csv_path\": csv_path}\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# TASK 5 â€” Load to Snowflake (from local)\n",
    "# -------------------------------------------------------------------\n",
    "def load_to_snowflake(ti):\n",
    "    csv_path = ti.xcom_pull(task_ids=\"transform_task\")\n",
    "    if not csv_path or not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"CSV not found at {csv_path}\")\n",
    "\n",
    "    snowflake_hook = SnowflakeHook(snowflake_conn_id=\"snowflake_default\")\n",
    "    conn = snowflake_hook.get_conn()\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    db = \"AIRFLOW_ETL_DATABASE\"\n",
    "    schema = \"PUBLIC\"\n",
    "    table = \"SP500_TABLE\"\n",
    "    target_table_fqn = f\"{db}.{schema}.{table}\"\n",
    "\n",
    "    try:\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE OR REPLACE TEMPORARY STAGE airflow_stage\n",
    "            FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"' SKIP_HEADER=1);\n",
    "        \"\"\")\n",
    "        cursor.execute(f\"PUT file://{csv_path} @airflow_stage AUTO_COMPRESS=FALSE;\")\n",
    "        cursor.execute(f\"\"\"\n",
    "            COPY INTO {target_table_fqn}\n",
    "            FROM @airflow_stage\n",
    "            FILE_FORMAT = (TYPE = 'CSV' \n",
    "            FIELD_DELIMITER = ','\n",
    "            FIELD_OPTIONALLY_ENCLOSED_BY='\"' \n",
    "            SKIP_HEADER=1)\n",
    "            ON_ERROR = 'CONTINUE';\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        print(\"âœ… Data uploaded to Snowflake successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Snowflake load failed: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# DAG DEFINITION\n",
    "# -------------------------------------------------------------------\n",
    "with DAG(\n",
    "    dag_id=\"SP500_Symbol_data_DAG\",\n",
    "    description=\"ETL for S&P 500 data to S3 + Snowflake\",\n",
    "    start_date=datetime(2025, 1, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    default_args={\"retries\": 1, \"retry_delay\": timedelta(minutes=5)},\n",
    "    tags=[\"sp500\", \"etl\", \"finance\"],\n",
    ") as dag:\n",
    "\n",
    "    extract_task_first = PythonOperator(\n",
    "        task_id=\"extract_task_first\",\n",
    "        python_callable=getSp500tickers,\n",
    "    )\n",
    "\n",
    "    extract_task_second = PythonOperator(\n",
    "        task_id=\"extract_task_second\",\n",
    "        python_callable=get_dataFromYf,\n",
    "    )\n",
    "\n",
    "    transform_task = PythonOperator(\n",
    "        task_id=\"transform_task\",\n",
    "        python_callable=transform,\n",
    "    )\n",
    "\n",
    "    load_task_s3 = PythonOperator(\n",
    "        task_id=\"load_task_s3\",\n",
    "        python_callable=load_to_s3,\n",
    "    )\n",
    "\n",
    "    load_task_snowflake = PythonOperator(\n",
    "        task_id=\"load_task_snowflake\",\n",
    "        python_callable=load_to_snowflake,\n",
    "    )\n",
    "\n",
    "    extract_task_first >> extract_task_second >> transform_task >> [load_task_s3, load_task_snowflake]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e7e5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebe0d8dc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
